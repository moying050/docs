---
title: bLSM论文阅读笔记
tags: 高并发,数据库,基础算法
grammar_cjkRuby: true
---
[TOC]
# 概要
现有数据管理的数据结构都是牺牲读性能来提高写性能。

bLSM比现有的LSM树：
- 在读方面性能更好
- 其中的合并调度器不会降低写吞吐，也不会阻塞写

布隆过滤器也在bSLM中使用，以提高索引查找性能：
- 找到一个版本的记录后读就返回，而不会读所有版本
- 如果在插入前直接到磁盘中查找数据会导致磁盘寻到，性能差

# 介绍
现代网络服务依赖两种小文件存储类型：
- update-in-place：要求随机读以及可接受的写延迟；一般用于人机交互的应用
- 用于分析型业务，追求写吞吐以及顺序读

由于业务的不同，最后后台会维护两套不同类型的存储，增加了各种成本，也影响到了终端用户的行为。

当用户行为不可预测时，就不能为不同应用准备不同类型的存储了。

# 背景
## 术语
测试不同数据结构读写性能中：
- 对于读的测试是通过seek来实现
- 对于写的测试是通过吞吐来衡量的

```math
read \, amplification = worst \, case seeks \, per \, index \, probe

write \, amplification = \frac{
total \, seq.I/O for \, object}{object \, size}

read \, fanout = \frac{data \, size}{RAM \, used \, by \, index}
```

## B-树
更新数据时，B-树提供了优化过的随机读能力。当一块在磁盘中的数据需要访问时，会触发一次磁盘seek，当需要修改该数据时，则需要2次磁盘seek。

hashmap也是类似的行为，由于没有排序，节省了内存，但扫描数据就不支持了。

磁盘数据传输速度为100-200MB/sec，平均访问时间为5ms。一个key/value对一般为1000字节，即1KB，则顺序写1个key/value对需要10us，寻道两次需要10ms，则读放大为1000。

## LSM-树
原始的LSM树：
- 有很高的读放大
- 不能利用写的局部性
- 在读时候会导致写可能被阻塞

虽然有这些问题，但原始LSM树的写放大很低，这些劣势因此也可接受。write skew（写偏序）通过树分区能解决。

### 基础算法
LSM树由一批磁盘上的追加B-树和一个内存中的索引树C0组成。数据的不断插入修改删除会触发这些树的扫描合并，最后把C0树中的一部分数据导出到磁盘树中。

磁盘树按键值排序，存储在磁盘中，这些文件都是有序的，因此查找会比比较快。读操作会检查文件，通过周期性的合并磁盘树来减少文件个数，提高查找性能。

**更新操作**

当更新请求被处理时，更新会被写到内存索引树，当内存索引树达到一定规模后，一部分数据会被刷到新的磁盘树（文件）。这些磁盘树文件代表了按时间顺序的修改。当然由于文件不会被修改，只会被追加写入，数据是有冗余的。

**读操作**

当一个读请求被处理时，系统首先在内存索引树中查找，如果没有新近的记录，则到磁盘上逆序查找（从新到旧），直到找到目标键值，效率为k*logN（k为磁盘树文件个数，N为每个文件的键值对数目）。

**读操作优化**

一般通过布隆过滤器来减少读文件的操作（判断不存在时就可以不去读某个磁盘树文件，以较少的内存空间获得高性价比的存在判断）。

**合并操作**

合并操作是将相邻时间的两个磁盘树文件做合并，具体没有展开。据说LevelDB的方案是这样：实现了一个分层的磁盘树文件集来执行合并操作。这个方法可以减少在最坏情况下需要检索的文件个数，同时也减少了一次合并操作的影响：
- 每一层可以维护指定的文件个数，同时保证不让key重叠。也就是说把key分区到不同的文件。因此在一层查找一个key，只用查找一个文件。第一层是特殊情况，不满足上述条件，key可以分布在多个文件中。
- 每次，文件只会被合并到上一层的一个文件。当一层的文件数满足特定个数时，一个文件会被选出并合并到上一层。这明显不同与另一种合并方式：一些相近大小的文件被合并为一个大文件。



# 算法改进
## 减少读放大
通过bloom过滤器和Fractional cascading两方面降低读的开销。

为系统中每个键值对分配10个比特做为bloom过滤器的内存占用开销，则通过bloom过滤器漏判的概率是1%，读取查找次数可以从N降低到1+N/100，评测显示内存大概会多占用5%左右，这还是划得来的。当然，对于扫描请求，bloom过滤器是没用的。

在基础算法中说道查找可能需要遍历所有磁盘树文件，效率为k*logN，如果使用Fractional cascading，则可以降低到k+logN，当然阶数不一样，复杂度有略微不同。

磁盘树文件越少，读放大越低，当只有1个磁盘树文件时，只需要一次seek，但此时写吞吐变低（需要读取的数据太多O(n)）。

**不写增量数据**

增量数据需要基础数据才可用，因此更新数据时将完整数据做RMW操作。

**对插入的优化**

bloom过滤器能加速键值不存在的情况，即如果通过bloom过滤器判断键不存在，则磁盘中肯定不存在该键，则插入能直接进行，不需要读取磁盘文件。这种场景的比例是很大的，因此优化效果不错。

**降低写停顿**

内存索引树满或者合并操作还没已完成，这些会阻塞写操作。

虽然bloom过滤器已经能过滤掉很多读操作，但对于扫描操作（范围查询等）则无能为力。

人工介入暂停合并操作等可以在系统压力高的时候执行，等压力低再开启合并操作，提高高峰时期系统吞吐能力。

另一种方案是把磁盘树文件做的比较小，这样即使阻塞写操作，这个时间也是很短的。但这种方案会造成写入不可预期的小停顿：
- 内存索引树C0已经满
- C1和C2磁盘树合并进度落后

**两次seek的扫描操作**
bSLM树有三个磁盘树文件，其中一个磁盘文件只是为了合并操作服务。使用分区后，磁盘树中只有一小部分会参与合并，剩余部分则只需要两次seek就能支持扫描操作。

# BLSM
算法优化的考虑优先级：
- 最高优先级是比B树性能更好（读和写）
- 相对原始LSM树，提高写的吞吐能力

## 齿轮调度器
加速调度器的引入是为了让合并操作更快以减少写操作被阻塞。

基础算法中，当一个磁盘树文件满的时候，算法会阻塞写操作直到合并操作完成。

这边使用时钟来类比算法，以C0/C1/C2为例，对应秒针分针时针：
- C0每个秒针单位把数据写到C1层的文件中
- C1层每分钟把C0写过来的文件做合并写到C2层
- C2层每小时进行文件做合并

在合并过程中，可能数据会变少，因此也不是每个分钟/时钟单位会有合并产生。

这种做法，每次触发合并的粒度都是固定的，即来自较小的C0的一部分数据。C1和C2需要做合并的数据也因此是固定大小的（或偏小），可以说是比较平滑的。

## 内存索引树的排序
排序需要考虑最小开销，最大开销，平均开销，内存需求，稳定性等。

对顺序输入友好，又兼顾随机输入，这里选择置换选择排序。

关于置换选择排序的算法可以看参考文档。

**LSM树合并的影响**

原始算法中，内存中有两个索引树，C0和C00，数据输入到C0，当C0需要导出到磁盘时会把C0的数据拷贝到C00，C00会用来做数据导出，从而减少对数据写的阻塞。但这样做会浪费一半的内存。

使用置换选择排序，内存利用率高，当内存快满时，每次写入都会顺序写一次磁盘（将输入写到磁盘中的文件），相当于C0的导出是细粒度做的，比较平滑。但如果数据没有做删除，一直insert的话，后期每次写入都会顺序写磁盘，性能比原始的查点。

结合前面的加速调度器来分析：
- 当内存没有满时，没有合并操作的发生，性能好；
- 当内存快满时，有合并操作的发生，会反过来降低写入的速度；
- 论文中的Figure6很形象，相当于拧发条：
    - 当内存空的多时，发条很松；
    - 当内存快满时，每次写入都会写磁盘，发条变紧，输出反馈到输入；
    - 当触发合并操作时，相当于发条更紧；
    - 但总体不会突然对输入做阻塞，而是可预期的，相对平滑的；

**分区的重要性**

置换选择排序没有C0的镜像，因此如果C1/C2只有1个，则正在合并时可能会阻塞用户操作。这么看前面的描述其实已经是分区过的算法，看来看论文的功力不够。

## 带发条的齿轮调度器
调度器在前述齿轮调度器基础上补充了两点（1点在前面其实已经描述）：
- C1/C2不是单个，而是一层，每层有很多个，降低合并粒度，降低用户操作被阻塞的程度
- C0的大小不是固定的，有一个上下限，低于下限不合并，低于上限开始做数据写磁盘，触到上限则把整个内存索引树导出到磁盘，会有用户操作的阻塞

这个设计的难点在于跟踪各层数的数目状态以及评估后续的合并操作的cpu/io消耗。：
- 简单的做法是只跟踪每层的文件数目以及键值数目
- 负载的做法是在简单做法基础上加入对write screw的考虑

## bLSM配置细节

**合并线程**

磁盘文件的迭代遍历操作需要访问磁盘，拿锁。为了提高性能，磁盘文件的迭代遍历操作需要尽量合并以减少不必要的消耗。

在合并时拿粗粒度的锁的行为对整体性能有很大影响。在不拿锁的情况下，合并线程需要获取其他线程的状态以作进一步操作。

**buffer管理以及恢复**

预写逻辑日志的使用可以用来支持事务以及数据库同步等。存在buffer是让对磁盘的输出先存到内存中，到一定时间再一起导出到磁盘，提高IO合并的程度。

由于LSM树有一部分数据是在内存中的，因此启动时需要回放checkpoint之后的日志，这里checkpoint指的是没有落盘的最老的日志，而且比最老日志新的数据中，有部分已经落盘了（置换选择排序的原因），因此日志要求是幂等性的。

**bloom过滤器**

每个磁盘文件一个bloom过滤器，该过滤器只会在磁盘文件数创建时被创建，磁盘树只会被追加删除，不会被修改，因此该过滤器不需要考虑并发update（并发update也是针对不同的key）。

bloom过滤器如果不持久化应该是简单的，如果需要持久化则会影响性能。将bloom过滤器的写回与合并操作放在同一个时刻是可接受的。

考虑到并发性，前后的合并操作相关的目标文件是不相关的。如何让并发的合并操作操作不同的磁盘文件需要分区。

# 性能测试
性能测试结果表明bLSM比levelDB在性能/时延上更优秀，磁盘的使用也更平滑（levelDB比较的版本比较老）。


# 参考文档
- [bLSM:∗ A General Purpose Log Structured Merge Tree]()
- [You could have invented fractional cascading](http://blog.ezyang.com/2012/03/you-could-have-invented-fractional-cascading/)
- [分散层叠（Fractional Cascading）](http://blog.csdn.net/baimafujinji/article/details/52956605)
- [Replacement Selection Sort v. Selection Sort](https://stackoverflow.com/questions/16326689/replacement-selection-sort-v-selection-sort)
- [理解数据库中的undo日志、redo日志、检查点](http://www.letiantian.me/2014-06-18-db-undo-redo-checkpoint/)